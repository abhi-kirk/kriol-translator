{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ffe8d51-d4d9-4cc6-aedd-bea0e001e9a1",
   "metadata": {},
   "source": [
    "# Seq2Seq Modeling\n",
    "- source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef69e640-f356-416a-b9ed-11ab6f72cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acb6ed9-ef6e-4e82-8442-e23914f2f6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence pairs: 9_395_786\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng-fra.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "print(f\"Number of sentence pairs: {len(data):_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047b371d-1206-46a3-80f0-e9baf71e01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ce92f-c135-4d04-8c2b-8a7f6e668a34",
   "metadata": {},
   "source": [
    "## Load data\n",
    "- Each word represented by a one-hot vector\n",
    "- Trim the vocab to only use a few thousand words per language\n",
    "- `Lang`: word-to-index and index-to-word dicts\n",
    "\n",
    "The full process for preparing the data is:\n",
    "- Read text file and split into lines, split lines into pairs\n",
    "- Normalize text, filter by length and content\n",
    "- Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a63266-276c-4def-8476-adc7c788f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # SOS, EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8648b498-6fa6-43b5-a982-f4c3435e000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f0e8a2-a62e-4ee2-8e8b-daf62971e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    lines = open(f\"{lang1}-{lang2}.txt\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e14947-1c87-4f7b-b5b8-d74e3ec08e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(\" \")) < MAX_LENGTH and \\\n",
    "        len(p[1].split(\" \")) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacfc63f-4484-4109-9215-ace8ba98ba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 pairs\n",
      "Trimmed to 11445 pairs\n",
      "Counting words..\n",
      "fra 4601\n",
      "eng 2991\n",
      "['nous allons toutes a la maison', 'we re all going home']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(f\"Read {len(pairs)} pairs\")\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(f\"Trimmed to {len(pairs)} pairs\")\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    print(\"Counting words..\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"eng\", \"fra\", True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6da05-54b8-4c86-b17a-e4dff9519799",
   "metadata": {},
   "source": [
    "## Encoder Model\n",
    "RNN: Creates a single vector out of the entire input sequence which, in the ideal case, encodes the 'meaning' of the input sequence into a single embedding vector. \n",
    "- Initial hidden state is zero if not explicitly provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfdbf27d-9fd2-4525-b64c-5b216f7b39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input_token):\n",
    "        embedded = self.embedding(input_token)  # create token embedding\n",
    "        embedded = self.dropout(embedded)  # apply dropout: randomly zeros some of the embedding vector's elements\n",
    "        output, hidden = self.gru(embedded)  # rnn: output is a collection of hidden at every time step\n",
    "        return output, hidden  # hidden is the final hidden vector capturing the meaning of the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5774b00-7fee-44ac-ba52-c1b945fbd1c8",
   "metadata": {},
   "source": [
    "## Decoder Model\n",
    "RNN: Takes the encoder output vector(s) and outputs a sequence of words to create the translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e31c3-fa9d-4e66-b3c0-185d189ce7cd",
   "metadata": {},
   "source": [
    "### Simple Decoder\n",
    "- Only use the last hidden state of the encoder, called the context vector as it encodes the context from the entire sequence.\n",
    "- Context vector used as the initial hidden state of the decoder.\n",
    "- Initial input token is the `<SOS>` token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56718190-9df6-4d41-8a58-8bff3b89cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # 'teacher forcing' from translated ground truth text (training)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)  # use its own predictions as the next input (inference)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input_token, hidden):\n",
    "        embedded = self.embedding(input_token)\n",
    "        embedded = F.relu(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2e2df-c658-4f9e-abe3-1d7af5d12606",
   "metadata": {},
   "source": [
    "### Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caf05911-4419-4103-9884-22363f2e2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fddeaa90-c50c-4ffd-b309-2c5a4093fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_type=\"general\"):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.attention_type = attention_type\n",
    "\n",
    "        if self.attention_type == \"general\":\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        if self.attention_type == \"dot\":\n",
    "            scores = torch.bmm(query, keys.transpose(1, 2))\n",
    "        elif self.attention_type == \"general\":\n",
    "            scores = torch.bmm(self.Wa(query), keys.transpose(1, 2))\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db3f89-5c5a-4467-8861-4ab87c3da06a",
   "metadata": {},
   "source": [
    "### Attention Decoder\n",
    "- Unlike simple decoder which only uses the last hidden state at each time step, attention decoder uses all hidden states to focus on different parts of the encoder's output.\n",
    "- Attention weights are multiplied by the encoder output vectors to create a weighted combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b80df6d2-f3fa-4dad-9dff-374df2fadc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # 'teacher forcing' from translated ground truth text (training)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)  # use its own predictions as the next input (inference)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        context, attn_weights = self.attention(\n",
    "            query=hidden.permute(1, 0, 2), \n",
    "            keys=encoder_outputs\n",
    "        )\n",
    "        output, hidden = self.gru(torch.cat((embedded, context), dim=2), hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdb12d-80ce-40a9-bac2-8a2d19b7ec86",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969d630c-43f4-49c4-93cb-0d6de954a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eee6dc9b-8311-4bc5-8446-71ea78ce667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData(\"eng\", \"fra\", True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(\n",
    "        torch.LongTensor(input_ids).to(device), \n",
    "        torch.LongTensor(target_ids).to(device)\n",
    "    )\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f40c8e5f-0237-447a-abc3-f69d73bffdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)), \n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "081826b0-c344-4216-96fa-7be902115647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c50811d-634a-4961-b04d-cfe8b4e2d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()  # negative log-likelihood loss\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss = train_epoch(\n",
    "            train_dataloader, \n",
    "            encoder, \n",
    "            decoder, \n",
    "            encoder_optimizer, \n",
    "            decoder_optimizer, \n",
    "            criterion\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f\"{timeSince(start, epoch / n_epochs)} ({epoch} {epoch / n_epochs * 100}%) {print_loss_avg:.4f}\")\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02d85d1b-b4d3-47d7-b018-3b41ece562f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 pairs\n",
      "Trimmed to 11445 pairs\n",
      "Counting words..\n",
      "fra 4601\n",
      "eng 2991\n",
      "1m 13s (- 18m 29s) (5 6.25%) 1.5446\n",
      "2m 26s (- 17m 5s) (10 12.5%) 0.6897\n",
      "3m 37s (- 15m 41s) (15 18.75%) 0.3568\n",
      "4m 49s (- 14m 27s) (20 25.0%) 0.1959\n",
      "5m 58s (- 13m 7s) (25 31.25%) 0.1203\n",
      "7m 9s (- 11m 56s) (30 37.5%) 0.0825\n",
      "8m 22s (- 10m 46s) (35 43.75%) 0.0628\n",
      "9m 35s (- 9m 35s) (40 50.0%) 0.0520\n",
      "10m 45s (- 8m 21s) (45 56.25%) 0.0449\n",
      "11m 53s (- 7m 8s) (50 62.5%) 0.0390\n",
      "13m 2s (- 5m 55s) (55 68.75%) 0.0370\n",
      "14m 9s (- 4m 43s) (60 75.0%) 0.0341\n",
      "15m 18s (- 3m 31s) (65 81.25%) 0.0326\n",
      "16m 27s (- 2m 21s) (70 87.5%) 0.0312\n",
      "17m 35s (- 1m 10s) (75 93.75%) 0.0296\n",
      "18m 44s (- 0m 0s) (80 100.0%) 0.0291\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, n_epochs=80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec064a-fa9a-48c5-a212-dffd4de14e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c2c18-c859-4a41-aacb-b30a37dccfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
